# -*- coding: utf-8 -*-
"""CloudNet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OOl9jGHwCV4I9NA4jgwAAFs72uqKtltT
"""

import torch
import numpy as np
import time
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import matplotlib.pyplot as plt
from PIL import Image, ImageOps
from google.colab import drive
drive.mount('/content/gdrive')

#resize and black pad images to 224x224 with no distortion
def cleaner(file):
    #resize image while keeping aspect ratio (e.g. 500x375 becomes 224x168)
    x = 224
    y = 224
    image = Image.open(file).convert("RGB")
    image.thumbnail((x, y), Image.ANTIALIAS)

    #add symmetrical black padding to the smaller dimension to match the desired x and y
    deltaX = x - image.size[0]
    deltaY = y - image.size[1]
    padding = (deltaX//2, deltaY//2, deltaX-(deltaX//2), deltaY-(deltaY//2))

    return ImageOps.expand(image, padding)

#load images from Google Drive
def loadImages(folder):
    #transform to tensor and normalize
    transform = transforms.Compose([transforms.ToTensor(), 
                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                    std=[0.229, 0.224, 0.225])])

    #load data from Google Drive and clean them as they load
    dataset = torchvision.datasets.ImageFolder(root=folder, loader=cleaner, transform=transform)
    
    return dataset

#verify that the images have been loaded and labeled
def verifyImages(dataset):
    #prepare dataloader
    dataLoader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=1, shuffle=True)

    #verification step - obtain one batch of images
    dataiter = iter(dataLoader)
    images, labels = dataiter.next()
    images = images.numpy() #convert images to numpy for display
    classes = ["Cloudy", "Not Cloudy"]

    #plot the images in the batch, along with the corresponding labels
    fig = plt.figure(figsize=(25, 4))
    for idx in np.arange(6):
        ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        picture = np.transpose(images[idx], (1, 2, 0))
        picture = std * picture + mean
        picture = np.clip(picture, 0, 1)
        plt.imshow(picture)
        ax.set_title(classes[labels[idx]])

#load data
data=loadImages('/content/gdrive/My Drive/APS360_Project/cloudData')
verifyImages(data)

#@title
#do not run
one_index,zero_index=0,0
for index in range(len(data)):
  #print(index)
  if (data.__getitem__(index)[1] == 0):
    zero_index+=1
  if (data.__getitem__(index)[1] == 1):
    one_index+=1
print(zero_index)
print(one_index)

def get_data_loader(batch_size=32):
  np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling
  indices=np.arange(3160)
  np.random.shuffle(indices)
  split = int(len(data) * 0.6)
  split2=int(len(data)*0.2)
  indices.size
  train_indices, val_indices, test_indices = indices[:split], indices[split:split+split2], indices[split+split2:]
  train_sampler = SubsetRandomSampler(train_indices)
  train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,
                                                num_workers=1, sampler=train_sampler)
  val_sampler = SubsetRandomSampler(val_indices)
  val_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,
                                                num_workers=1, sampler=val_sampler)
  test_sampler = SubsetRandomSampler(test_indices)
  test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,
                                                num_workers=1, sampler=test_sampler)
  return train_loader,val_loader,test_loader

import torchvision.models
resnet50 = torchvision.models.resnet50(pretrained=True) #load ResNet50

torch.__version__
#!pip install -q torch==0.4.1 torchvision
from torchvision import datasets, models, transforms
import torchvision
torchvision.__version__  # should be 0.2.1
input_path = "/content/gdrive/My Drive/APS360_Project/data"

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

model = models.resnet50(pretrained=True).to(device)
    
for param in model.parameters():
    param.requires_grad = False   
    
model.fc = nn.Sequential(
               nn.Linear(2048, 32),#3208
               nn.ReLU(inplace=True),
               nn.Linear(32, 2)).to(device)
               #nn.Linear(32, 2))
model.

#Generate a name for the model consisting of all the hyperparameter values
def get_model_name(size=320, batchSize=500, learningRate=0.0008, epoch=29):
    return "/content/gdrive/My Drive/APS360_Project/CloudNet/Models/size{0}_bs{1}_lr{2}_epoch{3}".format(size, batchSize, learningRate, epoch)

#criterion = nn.CrossEntropyLoss()
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.fc.parameters())

def get_accuracy(model, train=False):
    if train:
        data = train_loader
    else:
        data = val_loader

    correct = 0
    total = 0
    for imgs, labels in data:
        if use_cuda and torch.cuda.is_available():
          imgs = imgs.cuda()
          labels = labels.cuda()
        output = model(imgs)
        
        #select index with maximum prediction score
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(labels.view_as(pred)).sum().item()
        total += imgs.shape[0]
    return correct / total
def train(model, train_loader, val_loader, batch_size=27, num_epochs=1, learn_rate = 0.01):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learn_rate)

    epochs, losses, train_acc, val_acc, iters = [], [], [], [], []

    # training
    print ("Training Started...")
    n = 0 # the number of iterations
    start_time = time.time()
    for epoch in range(num_epochs):
        epochs.append(epoch)
        total_train_loss = 0.0
        total_train_err = 0.0
        total_epoch = 0
        for imgs, labels in iter(train_loader):
            
            if use_cuda and torch.cuda.is_available():
              imgs = imgs.cuda()
              labels = labels.cuda()

            out = model(imgs)             # forward pass
            loss = criterion(out, labels) # compute the total loss
            loss.backward()               # backward pass (compute parameter updates)
            optimizer.step()              # make the updates for each parameter
            optimizer.zero_grad()         # a clean up step for PyTorch
            iters.append(n) #Keep track of iterations
            n += 1       
        
        # track accuracy
        train_acc.append(get_accuracy(model, train=True))
        val_acc.append(get_accuracy(model, train=False))
        losses.append(loss)
        print("Epoch %d; Loss %f; Train Acc %f; Val Acc %f" % (
              epoch+1, loss, train_acc[-1], val_acc[-1]))
        
##################################################################
        # Save the current model (checkpoint) to a file
        model_path = get_model_name(batch_size, learn_rate, num_epochs)
        torch.save(model.state_dict(), model_path)
    print('Finished Training')
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("Total time elapsed: {:.2f} seconds".format(elapsed_time))
    # Write the train/test loss/err into CSV file for plotting later
    epochs = np.arange(1, num_epochs + 1)
    np.savetxt("{}_train_acc.csv".format(model_path), train_acc)
    np.savetxt("{}_val_acc.csv".format(model_path), val_acc)
    np.savetxt("{}_losses.csv".format(model_path), losses)
####################################################################
    # plotting
    plt.title("Training Curve")
    plt.plot(losses, label="Train")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()

    plt.title("Training Curve")
    plt.plot(epochs, train_acc, label="Train")
    plt.plot(epochs, val_acc, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.show()
    return train_acc, val_acc, iters

train_loader, val_loader, test_loader = get_data_loader(32)
use_cuda = True

if use_cuda and torch.cuda.is_available():
  model.cuda()
  print('CUDA is available!  Training on GPU ...')
else:
  print('CUDA is not available.  Training on CPU ...')
train(model, train_loader, val_loader, batch_size=32, num_epochs=30, learn_rate = 0.01)

batch_size= 30
train_loader, val_loader, test_loader = get_data_loader(batch_size)
use_cuda = True

if use_cuda and torch.cuda.is_available():
  model.cuda()
  print('CUDA is available!  Training on GPU ...')
else:
  print('CUDA is not available.  Training on CPU ...')
train(model, train_loader, val_loader, batch_size=batch_size, num_epochs=50, learn_rate = 0.0005)

#from ImageCleaning import *
import torch
import torch.nn as nn
#using the trained neural network, evaluate its performance on a data set
def evaluate(net, loader, criterion=nn.BCEWithLogitsLoss(), testing=False, showImages=False):
    totalLoss = 0
    totalAcc = 0
    total0Acc = 0
    total1Acc = 0
    total0 = 0
    total1 = 0
    totalEpoch = 0

    for i, data in enumerate(loader, 0):
        #get the inputs
        inputs, labels = data

        if showImages:
            images = inputs.numpy() #convert images to numpy for display
        
        if torch.cuda.is_available():
            inputs = inputs.cuda()
            labels = labels.cuda()

        #get the predictions
        outputs = net(inputs)

        #evaluate the loss and accuracy
        loss = criterion(outputs, labels.float())
        cacc = (outputs > 0).squeeze().long() == labels
        
        #evaluate true positives and true negatives
        cfalse = 0
        ctrue = 0
        if testing:
            for i, output in enumerate(outputs.squeeze(), 0):
                if labels[i].item() == 0:
                    total0 += 1
                    if output.item() <= 0:
                        cfalse += 1
                elif labels[i].item() == 1:
                    total1 += 1
                    if output.item() > 0:
                        ctrue += 1

        totalAcc += int(cacc.sum())
        totalLoss += loss.item()
        totalEpoch += len(labels)
        total0Acc += cfalse
        total1Acc += ctrue
        
        #print the images, their labels and whether the model was correct or not
        if showImages:
            classes = ["cloudy", "notCloudy"]
            #plot the pictures, labels and if the model got it right
            fig = plt.figure(figsize=(25,  4 * (len(labels)//10+1)))
            for idx in range(len(labels)):
                ax = fig.add_subplot(len(labels)//10+1, 10, idx+1, xticks=[], yticks=[])
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                picture = np.transpose(images[idx], (1, 2, 0))
                picture = std * picture + mean
                picture = np.clip(picture, 0, 1)
                plt.imshow(picture)
                guess = int(outputs.squeeze()[idx].item() > 0)
                if guess == labels[idx].item():
                    isCorrect = ' O'
                else:
                    isCorrect = ' X'
                ax.set_title(classes[labels[idx].item()] + isCorrect)
    
    #calculate the final accuracy and loss
    acc = float(totalAcc) / totalEpoch
    loss = float(totalLoss) / (i + 1)
    if not testing:
        return acc, loss
    else:
        return acc, total0Acc / total0, total1Acc / total1

#example of usage
#IMPORTANT: if you saved ResNet50 features to Google Drive
#you have to load IMAGES instead of those features and feed it to
#resnet50, then to the rest of your model's layers

#Generate a name for the model consisting of all the hyperparameter values
def getModelName(size=320, batchSize=500, learningRate=0.0008, epoch=29):
    return "/content/gdrive/My Drive/APS360_Project/CloudNet/Models/size{0}_bs{1}_lr{2}_epoch{3}".format(size, batchSize, learningRate, epoch)

#acc, acc0, acc1 = evaluate(model, train_loader, nn.BCEWithLogitsLoss(), False True)

#imagePath = "/content/gdrive/My Drive/Colab Notebooks/ProjectData/SnowData/"
#testingImages = loadImages(imagePath + "Testing")
#testLoader = torch.utils.data.DataLoader(testingImages, batch_size=batchSize, num_workers=1, shuffle=True)

#
#print(acc)
#print(acc0)
#print(acc1)

class cloudClassifier(nn.Module):
    def __init__(self, fcSize, getFeatures=False, useCuda=True):
        self.name = "cloudClassifier"
        self.size = fcSize
        self.getFeatures = getFeatures
        self.useCuda = useCuda
        super(cloudClassifier, self).__init__()
        self.fc1 = nn.Linear(1000, self.size)
        self.fc2 = nn.Linear(self.size, self.size//2)
        self.fcf = nn.Linear(self.size//2, 1)

    def forward(self, x):
        if self.getFeatures:
            if self.useCuda:
                x = resnet50.cuda()(x)
            else:
                x = resnet50(x)
        x = x.view(-1, 1000)
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = self.fcf(x)
        return x.squeeze(1)

def train_net(net,resnet50,batch_size=64, learning_rate=0.01, num_epochs=30):
    ########################################################################
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(net.parameters(), lr=learning_rate)
    
    train_loader, val_loader, test_loader = get_data_loader(batch_size)
    ########################################################################
    # Set up some numpy arrays to store the training/test loss/erruracy
    val_loss, train_loss, train_acc, val_acc, iters = [], [], [], [], []

    ########################################################################
    # Train the network
    # Loop over the data iterator and sample a new batch of training data
    # Get the output from the network, and optimize our loss function.
    start_time = time.time()
    n=0
    for epoch in range(num_epochs):  # loop over the dataset multiple times
        total_train_loss = 0.0
        total_train_err = 0.0
        total_epoch = 0
        for imgs, labels in iter(train_loader):
          if use_cuda and torch.cuda.is_available():
              imgs = imgs.cuda()
              labels = labels.cuda()
          out1=resnet50(imgs)
          out = net(out1)             # forward pass
          loss = criterion(out, labels.float()) # compute the total loss
          loss.backward()               # backward pass (compute parameter updates)
          optimizer.step()              # make the updates for each parameter
          optimizer.zero_grad()         # a clean up step for PyTorch
          iters.append(n) #Keep track of iterations
          n += 1       
          # Calculate the statistics
        train_tup = evaluate(net,train_loader)
        val_tup = evaluate(net,val_loader)
        train_acc.append(train_tup[0])
        val_acc.append(val_tup[0])
        train_loss.append(train_tup[1])
        val_loss.append(val_tup[1])
        print(("Epoch {}: Train accuracy: {}, Train loss: {} |"+
               "Validation accuracy: {}, Validation loss: {}").format(
                   epoch + 1,
                   train_acc[epoch],
                   train_loss[epoch],
                   val_acc[epoch],
                   val_loss[epoch]))
        # Save the current model (checkpoint) to a file
        model_path = getModelName(net.name, batch_size, learning_rate, epoch)
        torch.save(net.state_dict(), model_path)
    print('Finished Training')
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("Total time elapsed: {:.2f} seconds".format(elapsed_time))
    # Write the train/test loss/err into CSV file for plotting later
    epochs = np.arange(1, num_epochs + 1)
    np.savetxt("{}_train_err.csv".format(model_path), train_acc)
    np.savetxt("{}_train_loss.csv".format(model_path), train_loss)
    np.savetxt("{}_val_err.csv".format(model_path), val_acc)
    np.savetxt("{}_val_loss.csv".format(model_path), val_loss)
####################################################################
    # plotting
    plt.title("Training Curve")
    plt.plot(train_loss, label="Train")
    plt.plot(val_loss, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()

    plt.title("Training Curve")
    plt.plot(train_acc, label="Train")
    plt.plot(val_acc, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.show()
    return train_acc, val_acc, iters

size = 750

model = cloudClassifier(size, False, True)
if torch.cuda.is_available():
    model = model.cuda()
    print('CUDA is available!  Training on GPU ...')
use_cuda = True
train_net(model,resnet50,batch_size=32,learning_rate=0.001,num_epochs=30)
#torch.save(net.state_dict(), model_path)
#model.load_state_dict(torch.load(getModelName(size, batchSize, learningRate, epoch)))

size = 1000

model2 = cloudClassifier(size, True, True)
if torch.cuda.is_available():
    model2 = model2.cuda()
    print('CUDA is available!  Training on GPU ...')
use_cuda = True
train_net(model2,batch_size=32,learning_rate=0.01,num_epochs=30)

!ps -aux|grep python

!kill -4429

class cloudClassifier2(nn.Module):
    def __init__(self, fcSize, getFeatures=False, useCuda=True):
        self.name = "cloudClassifier2"
        self.size = fcSize
        self.getFeatures = getFeatures
        self.useCuda = useCuda
        super(cloudClassifier2, self).__init__()
        self.fc1 = nn.Linear(1000, self.size)
        #self.fc2 = nn.Linear(self.size, self.size//2)
        self.fcf = nn.Linear(self.size, 1)

    def forward(self, x):
        if self.getFeatures:
            if self.useCuda:
                x = resnet50.cuda()(x)
            else:
                x = resnet50(x)
        x = x.view(-1, 1000)
        x = F.leaky_relu(self.fc1(x))
        #x = F.leaky_relu(self.fc2(x))
        x = self.fcf(x)
        return x.squeeze(1)

size = 500

model2 = cloudClassifier(size, True, True)
if torch.cuda.is_available():
    model2 = model.cuda()
    print('CUDA is available!  Training on GPU ...')
use_cuda = True
train_net(model2,resnet50,batch_size=32,learning_rate=0.0001,num_epochs=30)

!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isn’t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print("Gen RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ), " | Proc size: " + humanize.naturalsize( process.memory_info().rss))
 print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()